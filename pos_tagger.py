# -*- coding: utf-8 -*-
"""INLP assignment -2 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kV--Xm87jUGl8bsIYOn_c8GbmKtxg9mf
"""

import torch
from torch import nn, tensor
from torch.utils.data import Dataset, DataLoader
from torchtext.vocab import build_vocab_from_iterator
from tqdm import tqdm
import torch.optim as optim
from torch.nn import functional as F
from torchvision import datasets
from torchvision.transforms import ToTensor
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import f1_score

import codecs

with codecs.open('/content/en_atis-ud-train.conllu', 'r', encoding='utf-8') as f:
    data = f.read()
col1 = []
col2 = []
col5 = []

# Spliting the data into sentences
sentences = data.split('\n\n')

for sentence in sentences:
    if sentence:
        lines = sentence.strip().split('\n')
        for line in lines:
            if line.startswith('#'):
                continue
            fields = line.strip().split('\t')
            if '-' in fields[0]:
                continue
            col1.append(fields[0])
            col2.append(fields[1])
            col5.append(fields[4])

df = pd.read_csv('/content/en_atis-ud-train.conllu', sep='\t', comment='#', header=None, names=['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC'], quoting=3, engine='python')

df2 = pd.read_csv('/content/en_atis-ud-train.conllu', sep='\t',  header=None, names=['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC'], quoting=3, engine='python')

train_data = []
x, y = [], []

for i in tqdm(range(len(df))):
  if df.iloc[i, 0] == 1:
      train_data.append((x, y))
      x, y = [], []
  else:
    x.append(df.iloc[i, 1])
    y.append(df.iloc[i, 3])

train_data = train_data[1:]
print(train_data[:2])

import pandas as pd

# Read the .conllu file into a pandas DataFrame
df2 = pd.read_csv('/content/en_atis-ud-train.conllu', sep='\t', names=['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC'], quoting=3, engine='python')

dict = {}
dict[1] = (df['FORM'])
dict[2] = (df['UPOS'])

len(dict[1])

for i in range (0,5):
  print(dict[1][i])

for i in range (0,5):
  print(dict[2][i])

train_set = set()
for i in range (0,len(dict[1])):
  train_set.add((dict[1][i],len(train_set)))

list_a = []
for i in range (0,len(dict[1])):
  list_a.append(dict[1][i])

my_set = set()
unique_values = {}

index = 0
for value in list_a:
    if value not in my_set:
        my_set.add(value)
        unique_values[value] = len(unique_values)
    index += 1

len(unique_values)

my_list = list(train_set)

my_list[1][1]

word_dict = {}
for i in range(len(my_list)):
  dict = {
      my_list[i][0] : my_list[i][1]
  }

dict

val_map = {'ADJ': 0, 'ADP': 1, 'ADV': 2, 'AUX': 3, 'CCONJ': 4, 'DET': 5, 'INTJ': 6,
           'NOUN': 7, 'NUM': 8, 'PART': 9, 'PRON': 10, 'PROPN': 11, 'VERB': 12, 'SYM': 13}

import numpy as np

def prepare_sequence(seq, to_idx):
    idxs = []
    for w in seq:
      if w not in to_idx:
        idxs.append(0)
        continue
      idxs.append(to_idx[w])
    # idxs = [to_idx[w] for w in seq]
    idxs = np.array(idxs)
    
    return torch.from_numpy(idxs)

example_input = prepare_sequence("level lga like list".split(),unique_values)

print(example_input)

class LSTMTagger(nn.Module):

    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):
        super(LSTMTagger, self).__init__()
        
        self.hidden_dim = hidden_dim

        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)

        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)
        
        self.hidden = self.init_hidden()

        
    def init_hidden(self):
        return (torch.zeros(1, 1, self.hidden_dim),
                torch.zeros(1, 1, self.hidden_dim))

    def forward(self, sentence):
        embeds = self.word_embeddings(sentence)
        lstm_out, self.hidden = self.lstm(
            embeds.view(len(sentence), 1, -1), self.hidden)
        
        tag_outputs = self.hidden2tag(lstm_out.view(len(sentence), -1))
        tag_scores = F.log_softmax(tag_outputs, dim=1)
        
        return tag_scores

EMBEDDING_DIM = 128
HIDDEN_DIM = 128

# instantiate our model
model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(unique_values), len(val_map))

# define our loss and optimizer
loss_function = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-2)

n_epochs = 20

for epoch in range(n_epochs):
    
    epoch_loss = 0.0
    
    # get all sentences and corresponding tags in the training data
    for sentence, tags in train_data:
        
        # zero the gradients
        model.zero_grad()

        # zero the hidden state of the LSTM, this detaches it from its history
        model.hidden = model.init_hidden()
        sentence_in = prepare_sequence(sentence, unique_values)
        # print(val_map)
        # print("huehuehue")
        # print(tags)
        targets = prepare_sequence(tags, val_map)

        # forward pass to get tag scores
        tag_scores = model(sentence_in)

        # compute the loss, and gradients 
        loss = loss_function(tag_scores, targets)
        epoch_loss += loss.item()
        loss.backward()
        
        # update the model parameters with optimizer.step()
        optimizer.step()
        
    # print out avg loss per 20 epochs
    if(epoch%2 == 0):
        print("Epoch: %d, loss: %1.5f" % (epoch+1, epoch_loss/len(train_data)))

PATH = "/content/pretrained_model.pt"
torch.save(model.state_dict(),PATH)

test_sentence = "what are the coach flights between dallas and baltimore leaving august tenth and returning august twelve".lower().split()

inputs = prepare_sequence(test_sentence, unique_values)
tag_scores = model(inputs)

predicted_tags = torch.argmax(tag_scores, 1)
print('\n')
print(predicted_tags)

"""**TESTING**"""

df3 = pd.read_csv('/content/en_atis-ud-test.conllu', sep='\t',  header=None, names=['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC'], quoting=3, engine='python')

# test_data = []
x_test, y_test = [], []
x, y = [], []

for i in tqdm(range(len(df3))):
  if df3.iloc[i, 1] != None:
    x.append(df3.iloc[i, 1])
    y.append(val_map[df3.iloc[i, 3]])
  else:
    if len(x) != 0:
      x_test.append(x)
      y_test.append(y)
      x,y = [], []

predictions = []

for x in tqdm(x_test):
  x = prepare_sequence(x, unique_values)
  pred = model(x)
  predictions.append(torch.argmax(pred, axis = 1))

acc = 0

for i in range(len(y_test)):
  acc += (predictions[i] == torch.tensor(y_test[i])).float().sum()/len(y_test[i])

acc/len(y_test)

f1 = 0

for i in range(len(y_test)):
  f1 += f1_score(y_test[i], predictions[i].tolist(), average = 'weighted')

f1 = f1/len(y_test)

print("F1 score: {:.5f}".format(f1))

"""**testing with the dev data**"""

df4 = pd.read_csv('/content/en_atis-ud-dev.conllu', sep='\t',  header=None, names=['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC'], quoting=3, engine='python')

df4

x_test, y_test = [], []
x, y = [], []

for i in tqdm(range(len(df4))):
  if df4.iloc[i, 1] != None:
    x.append(df4.iloc[i, 1])
    y.append(val_map[df4.iloc[i, 3]])
  else:
    if len(x) != 0:
      x_test.append(x)
      y_test.append(y)
      x,y = [], []

predictions = []

for x in tqdm(x_test):
  x = prepare_sequence(x, unique_values)
  pred = model(x)
  predictions.append(torch.argmax(pred, axis = 1))

acc = 0

for i in range(len(y_test)):
  acc += (predictions[i] == torch.tensor(y_test[i])).float().sum()/len(y_test[i])

acc/len(y_test)

# Assuming y_true and y_pred are your true and predicted labels
y_true = np.array([0, 1, 1, 0, 1])
y_pred = np.array([1, 1, 0, 0, 1])

# Calculate F1 score
f1 = f1_score(y_true, y_pred, average='weighted')

print(f1)

input_string = "Mary had a little lamb"
test_sentence = input_string.lower().split()

# see what the scores are after training
inputs = prepare_sequence(test_sentence, unique_values)
tag_scores = model(inputs)

# print the most likely tag index, by grabbing the index with the maximum score!
# recall that these numbers correspond to tag2idx = {"DET": 0, "NN": 1, "V": 2}
predicted_tags = torch.argmax(tag_scores, 1)
print('\n')
# print(predicted_tags)
my_array = predicted_tags.numpy()

# iterate over the array and print each element
# print(test_sentence)
for i in range(len(my_array)):
  
  if(my_array[i] == 0):
    a  = ('ADJ')
  elif(my_array[i] == 1):
    a = ('ADP')
  elif(my_array[i] == 2):
    a = ('ADV')
  elif(my_array[i] == 3):
    a = ('AUX')
  elif(my_array[i] == 4):
    a = ('CCNOJ')
  elif(my_array[i] == 5):
    a = ('DET')
  elif(my_array[i] == 6):
    a = ('INTJ')
  elif(my_array[i] == 7):
    a = ('NOUN')
  elif(my_array[i] == 8):
    a = ('NUM')
  elif(my_array[i] == 9):
    a = ('PART')
  elif(my_array[i] == 10):
    a = ('PRON')
  elif(my_array[i] == 11):
    a = ('PROPN')
  elif(my_array[i] == 12):
    a = ('VERB')
  print(test_sentence[i],'\t' ,a )